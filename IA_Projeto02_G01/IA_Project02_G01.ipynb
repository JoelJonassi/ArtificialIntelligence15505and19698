{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Machine Learning\n",
    "\n",
    "Students: Joel Jonassi 19698\n",
    "        Rui Alves 15505\n",
    "Teacher: Joaquim Silva"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet Link : https://www.kaggle.com/datasets/camnugent/california-housing-prices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For this job we will analyse the California house dataset...\n",
    "\n",
    "Will start with a contextualization about the metrics that we will use to evaluate the models.\n",
    "\n",
    "Note: Underfiting if the training error and error testing high\n",
    "Overfitting if error training is low or accurancy high and error testing low or accurancy high"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualization\n",
    "At this section will make a contextualization about the metrics used in this project to measure the models performance.\n",
    "\n",
    "### RandomForest and DecisionTreeClassifier\n",
    "#### Classification accuracy\n",
    "The classification accuracy is the ratio of number of correct predictions to the total number of input samples.\n",
    "This metric works well only if there are equal number of samples belonging to each class.\n",
    "\n",
    "According to the \"https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234\", says that if we consider that there are 98% samples of class A and 2% samples of class B in our training set. Then our model can easily get 98% training accuracy by simply predicting every training sample belonging to class A.\n",
    "When the same model is tested on a test set with 60% samples of class A and 40% samples of class B, then the test accuracy would drop down to 60%. Classification Accuracy is great, but gives us the false sense of achieving high accuracy.\n",
    "image \n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.\n",
    "There are 4 important terms :\n",
    "\n",
    "* True Positives : The cases in which we predicted YES and the actual output was also YES.\n",
    "* True Negatives : The cases in which we predicted NO and the actual output was NO.\n",
    "* False Positives : The cases in which we predicted YES and the actual output was NO.\n",
    "* False Negatives : The cases in which we predicted NO and the actual output was YES.\n",
    "Accuracy for the matrix can be calculated by taking average of the values lying across the “main diagonal”.\n",
    "\n",
    "image\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "F1 Score is used to measure a test’s accuracy\n",
    "\n",
    "F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n",
    "\n",
    "High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model. Mathematically, it can be expressed as :\n",
    "\n",
    "\n",
    "F1 Score tries to find the balance between precision and recall.\n",
    "image\n",
    "\n",
    "Precision : It is the number of correct positive results divided by the number of positive results predicted by the classifier.\n",
    "image\n",
    "Recall : It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).\n",
    "image\n",
    "\n",
    "\n",
    "#### Mean Absolute Error\n",
    "Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data. Mathematically, it is represented as :\n",
    "image\n",
    "\n",
    "#### Mean Squared Error\n",
    "Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values. The advantage of MSE being that it is easier to compute the gradient, whereas Mean Absolute Error requires complicated linear programming tools to compute the gradient. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, mean_absolute_error, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from mlxtend.frequent_patterns import association_rules, apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from apyori import apriori\n",
    "from sklearn import tree\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/housingDataPrepare.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetives:\n",
    " * Try do predict if the house is near to the beach or not"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "We consider the all  the dataset attributes important to train the model to predict if the house is near to the beach or not.\n",
    "We use a whole dataset composed of 10 attributes, 9 of which are for testing and 1 for the model to predict \"target attribute\":\n",
    "* latitude and longitude - we use this two variables to helps ous finding the location of the houses.\n",
    "* house_median_age - this attribute helps our model to predict the age of the house and try make a decision according to other attributes in the dataset if the house is near to the beach or not.\n",
    "* total_rooms and total - we find these variables important to train the model cause can tells ous how many rooms are near to the beach as we know near to the beach has many houses/rooms for tourism.\n",
    "* population - Can have a correlation between population and proximity to the beach\n",
    "* house_hold - we assume that the household can influence the proximity to the beach or not.\n",
    "* median_icome and median_house_value - these two variables we assume that are essential to predict if the house is near the beach cause people with with higher purchasing power are more likely to live there because the cost of living in these areas is expensive.\n",
    "* ocean_proximity - This is the target attribute, we will use two algorithms to predict the ocean proximity namely Decision Random Forest Algorithm and Tree Classifier.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean') \n",
    "\n",
    "\n",
    "data = df.iloc[: , :-3] # Ignore last 3 columns\n",
    "\n",
    "X = data.iloc[ :, : -1].values # Use all columns except de last one in \"data\"\n",
    "\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "Y = data.iloc[ :, 9].values # Target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=0)\n",
    "\n",
    "#Using random forest classifier\n",
    "classifier = RandomForestClassifier() \n",
    "classifier = classifier.fit(X_train, Y_train) # model creation\n",
    "predicted = classifier.predict(X_test) # model evaluation\n",
    "\n",
    "cols = ['ISLAND', 'NEAR OCEAN', 'NEAR BAY', '<1H OCEAN', 'INLAND']\n",
    "\n",
    "#Results\n",
    "print ('Confusion Matrix :')\n",
    "confm = confusion_matrix(Y_test, predicted, labels=cols)\n",
    "print(confm)\n",
    "print('Accuracy Score :', accuracy_score(Y_test, predicted))\n",
    "print('Report : ')\n",
    "print(classification_report(Y_test, predicted, labels=cols, zero_division=0))\n",
    "\n",
    "df_cm = DataFrame(confm, index=cols, columns=cols)\n",
    "ax = sns.heatmap(df_cm, cmap='Oranges', annot=True, fmt=\"d\")\n",
    "\n",
    "\n",
    "conv_to_num = {\n",
    "    'ISLAND': 1,\n",
    "    'NEAR OCEAN': 2,\n",
    "    'NEAR BAY': 3,\n",
    "    '<1H OCEAN': 4,\n",
    "    'INLAND': 5\n",
    "}\n",
    "\n",
    "def convert_num(name):\n",
    "    return conv_to_num[name]\n",
    "\n",
    "mae_y_true = list(map(convert_num, Y_test))\n",
    "mae_y_pred = list(map(convert_num, predicted))\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(mae_y_true, mae_y_pred)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier().fit(X_train, Y_train) # model creation\n",
    "predicted = DTC.predict(X_test) # model evaluation\n",
    "print('confusion Matrix')\n",
    "confm = confusion_matrix(Y_test, predicted, labels=cols)\n",
    "print(confm)\n",
    "print('Accuracy Score : ', accuracy_score(Y_test, predicted))\n",
    "print(\"Report: \")\n",
    "print(classification_report(Y_test, predicted))\n",
    "\n",
    "mae_y_true = list(map(convert_num, Y_test))\n",
    "mae_y_pred = list(map(convert_num, predicted))\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(mae_y_true, mae_y_pred)}\")\n",
    "\n",
    "df_cm = DataFrame(confm, index=cols, columns=cols)\n",
    "ax = sns.heatmap(df_cm, cmap='Oranges', annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"longitude\"\t,\"latitude\",\"housing_median_age\"\t,\"total_rooms\"\t,\"total_bedrooms\"\t,\"population\",\t\"households\",\t\"median_income\"\t,\"median_house_value\"]\n",
    "target_names = \"ocean_proximity\"\n",
    "plt.figure(figsize=(60,60))\n",
    "tree.plot_tree(DTC, filled=True, feature_names=feature_names, class_names=target_names,fontsize=6,)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "At this section we will discuss a classification Accuracy. \n",
    "\n",
    "\n",
    "## Analysis\n",
    "As we can see int the confusion matrix of these two algorithms RandomForest and DecisionTree it´s clearly that one is better than the other.\n",
    "### Root of Mean Square Error (`rms_error`)\n",
    "\n",
    "This is the square root of `Mean Square Error`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster with K-Means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetives:\n",
    " * Classify the zones where the people with the highest purchasing power reside by ticket price. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "X = df.loc[:, [\"median_house_value\", \"latitude\", \"longitude\"]]\n",
    "X.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster feature\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot that shows the geographic distribution of the clusters. It seems like the algorithm has created separate segments for higher-income areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    x=\"longitude\", y=\"latitude\", hue=\"Cluster\", data=X, height=6,\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target in this dataset is median_house_value (median house value). These box-plots show the distribution of the target within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"median_house_value\"] = df[\"median_house_value\"]\n",
    "sns.catplot(x=\"median_house_value\", y=\"Cluster\", data=X, kind=\"boxen\", height=9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules\n",
    "\n",
    "Objetives:\n",
    "* Knowing whether or not you are close to the beach, determine the price of the house.\n",
    "\n",
    "### Data Prepare\n",
    "Add numerical column for ocean proximity\n",
    "=SE(H2<=2,\"Very Low Income\",SE(E(2<H2,H2<=4),\"Low Income\",SE(E(4<H2,H2<=6),\"Medium Income\",SE(E(6<H2,H2<=8),\"High Income\",\"Very High Income\"))))\n",
    "=SE(I2<=11,\"Very Recent House\",SE(E(11<I2,I2<=22),\"Recent House\",SE(E(22<I2,I2<=33),\"Medium Aged House\",SE(E(33<I2,I2<=44),\"Older House\",\"Very Older House\"))))\n",
    "=SE(I2<=90000,\"Very Cheap House\",SE(E(90000<I2,I2<=180000),\"Cheap House\",SE(E(180000<I2,I2<=270000),\"Medium Priced House\",SE(E(270000<I2,I2<=360000),\"Expensive House\",\"Very Expensive House\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Segragation\n",
    "data = df.iloc[: , :] # All data\n",
    "data = df.iloc[:, 9 :]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in range(0, len(data)):\n",
    "    records.append([str(data.values[i, j]) for j in range(0, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=3) # creation of the model\n",
    "association_results = list(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in association_results:\n",
    "    pair = item[0]\n",
    "    items = [x for x in pair]\n",
    "    if(len(items) == 2):\n",
    "        print(\"Rule:\" + items[0] + \"->\" + items[1])\n",
    "        print(\"Suport: \"+ str(item[1]))\n",
    "        print(\"Confidence: \"+ str(item[2][0][2]))\n",
    "        print(\"Lift: \" + str(item[2][0][3]))\n",
    "print(\"Total number of rules mined = \" , len(association_results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "\n",
    "Rule: High Income -> Expensive House\n",
    "Suport : \n",
    "Confidence : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "4b012d59fd9967520aa1876644e3a9774eee391dbb6584df68e553498820aaf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
